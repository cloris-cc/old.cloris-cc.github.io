<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>TensorFlow - 茶茶日记 - winklog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="茶茶日记 - winklog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="茶茶日记 - winklog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="…"><meta property="og:type" content="blog"><meta property="og:title" content="TensorFlow"><meta property="og:url" content="https://teamwang.cn/2021/09/03/TensorFlow/"><meta property="og:site_name" content="茶茶日记 - winklog"><meta property="og:description" content="…"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/a03be577-3fec-4098-935a-187745943817.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/176dbf0f-1408-4ce7-9c9e-79f905cbd630.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/d44d0b00-bcbc-4436-98e0-e4d3d28cb61d.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/484bb6c3-efa2-4106-9e01-96215491f2d6.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/34c9ef7d-875a-4b93-b452-9699817e74a4.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/b40d8531-b315-4d6d-8a4d-1bf85d727fcc.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/0d0b9db0-97c8-41f0-9d01-8f800f94296c.jpg"><meta property="og:image" content="https://pic3.zhimg.com/80/v2-d4563539367d5cfcc3ff749eb309d45a_1440w.jpg"><meta property="og:image" content="https://pic3.zhimg.com/80/v2-28aeed7f2b1363dfbd3c55cc2360868e_1440w.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/9YRKFZ.png"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/Q1M5Dj.png"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/V5FA0J.png"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/74CNoT.png"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/97c1df15-263b-476a-8b1f-914e3d77e667.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/4f1f437b-9aee-49a0-b4b3-bf8042d9e69b.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/c4340a6f-332c-4866-8eb8-5ed76835a4e5.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/e5dd865c-7634-47af-a32a-6daa5f83c2d3.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/6ea95886-2432-4644-b7dd-5675183d11b2.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/231b174f-5c32-427f-8927-945bf0065613.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/297faa82-c1e4-4ed7-b7a8-4698da23c001.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/rFPtQR.png"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/cb2630be-bf11-4045-98b7-cf91354ca9ed.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/1ff76ca3-b5cc-4624-8540-780b492f6e0d.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/e46dab09-d0f3-453d-9823-565da16c98c1.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/00ede66f-5ce3-444c-a323-9cb1c027f9f7.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/ea684877-1a0c-4fd5-a3f5-f037508ddf23.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/lpmGcn.png"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/a6d00b83-1994-4fe7-bdfc-bce6e9ee68a7.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/02fd8c12-9cf5-4278-827e-0b7b32691cb0.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/792f1e1f-b5cd-4ff5-a90b-0deb93578f91.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/54ca4ddb-9fbf-4ded-a517-dac94e31fb69.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/a01324c7-fb09-44a2-91c5-4c24391ab722.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/3d804594-4599-4640-ac5d-534efb74443d.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/d816ff44-ac35-4289-a083-a62e77b1390e.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/f01c1061-277c-4bde-bcf9-7021aa1db659.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/c12a1b21-df1c-485b-a362-32a4631552d4.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/ca0b9d7f-12e8-4d59-b2be-c1abd4aa23a7.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/0a291647-0c1a-4a62-9eb4-306554975f3e.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/67e43065-88ff-4e0e-ab0e-65c70676156b.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/167f9296-e34d-478c-828c-deca8b2a404d.jpg"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/bF4W8x.png"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/84b6678c-1988-47e1-a724-cce1b52165fd.png"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/g2zZqu.png"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/shhpOQ.png"><meta property="og:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/tXty8y.png"><meta property="article:published_time" content="2021-09-03T06:59:11.000Z"><meta property="article:modified_time" content="2021-09-03T07:02:47.759Z"><meta property="article:author" content="Jacky"><meta property="article:tag" content="茶茶,日记,茶茶日记,winklog,WinkLog,博客,teamwang,TeamWang,王嘉尔,Jakcy,jacky"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://gitee.com/jacky_cloud/oss/raw/master/uPic/a03be577-3fec-4098-935a-187745943817.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://teamwang.cn/2021/09/03/TensorFlow/"},"headline":"TensorFlow","image":["https://gitee.com/jacky_cloud/oss/raw/master/uPic/a03be577-3fec-4098-935a-187745943817.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/176dbf0f-1408-4ce7-9c9e-79f905cbd630.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/d44d0b00-bcbc-4436-98e0-e4d3d28cb61d.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/484bb6c3-efa2-4106-9e01-96215491f2d6.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/34c9ef7d-875a-4b93-b452-9699817e74a4.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/b40d8531-b315-4d6d-8a4d-1bf85d727fcc.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/0d0b9db0-97c8-41f0-9d01-8f800f94296c.jpg","https://pic3.zhimg.com/80/v2-d4563539367d5cfcc3ff749eb309d45a_1440w.jpg","https://pic3.zhimg.com/80/v2-28aeed7f2b1363dfbd3c55cc2360868e_1440w.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/9YRKFZ.png","https://gitee.com/jacky_cloud/oss/raw/master/uPic/Q1M5Dj.png","https://gitee.com/jacky_cloud/oss/raw/master/uPic/V5FA0J.png","https://gitee.com/jacky_cloud/oss/raw/master/uPic/74CNoT.png","https://gitee.com/jacky_cloud/oss/raw/master/uPic/97c1df15-263b-476a-8b1f-914e3d77e667.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/4f1f437b-9aee-49a0-b4b3-bf8042d9e69b.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/c4340a6f-332c-4866-8eb8-5ed76835a4e5.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/e5dd865c-7634-47af-a32a-6daa5f83c2d3.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/6ea95886-2432-4644-b7dd-5675183d11b2.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/231b174f-5c32-427f-8927-945bf0065613.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/297faa82-c1e4-4ed7-b7a8-4698da23c001.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/rFPtQR.png","https://gitee.com/jacky_cloud/oss/raw/master/uPic/cb2630be-bf11-4045-98b7-cf91354ca9ed.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/1ff76ca3-b5cc-4624-8540-780b492f6e0d.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/e46dab09-d0f3-453d-9823-565da16c98c1.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/00ede66f-5ce3-444c-a323-9cb1c027f9f7.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/ea684877-1a0c-4fd5-a3f5-f037508ddf23.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/lpmGcn.png","https://gitee.com/jacky_cloud/oss/raw/master/uPic/a6d00b83-1994-4fe7-bdfc-bce6e9ee68a7.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/02fd8c12-9cf5-4278-827e-0b7b32691cb0.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/792f1e1f-b5cd-4ff5-a90b-0deb93578f91.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/54ca4ddb-9fbf-4ded-a517-dac94e31fb69.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/a01324c7-fb09-44a2-91c5-4c24391ab722.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/3d804594-4599-4640-ac5d-534efb74443d.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/d816ff44-ac35-4289-a083-a62e77b1390e.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/f01c1061-277c-4bde-bcf9-7021aa1db659.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/c12a1b21-df1c-485b-a362-32a4631552d4.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/ca0b9d7f-12e8-4d59-b2be-c1abd4aa23a7.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/0a291647-0c1a-4a62-9eb4-306554975f3e.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/67e43065-88ff-4e0e-ab0e-65c70676156b.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/167f9296-e34d-478c-828c-deca8b2a404d.jpg","https://gitee.com/jacky_cloud/oss/raw/master/uPic/bF4W8x.png","https://gitee.com/jacky_cloud/oss/raw/master/uPic/84b6678c-1988-47e1-a724-cce1b52165fd.png","https://gitee.com/jacky_cloud/oss/raw/master/uPic/g2zZqu.png","https://gitee.com/jacky_cloud/oss/raw/master/uPic/shhpOQ.png","https://gitee.com/jacky_cloud/oss/raw/master/uPic/tXty8y.png"],"datePublished":"2021-09-03T06:59:11.000Z","dateModified":"2021-09-03T07:02:47.759Z","author":{"@type":"Person","name":"Jacky"},"description":"…"}</script><link rel="canonical" href="https://teamwang.cn/2021/09/03/TensorFlow/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?05af3f3bd18ffe652b63cc7ee6abb48c";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="茶茶日记 - winklog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">AboutMe</a></div><div class="navbar-end"><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-09-03T06:59:11.000Z" title="9/3/2021, 2:59:11 PM">2021-09-03</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-09-03T07:02:47.759Z" title="9/3/2021, 3:02:47 PM">2021-09-03</time></span><span class="level-item"><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">41 minutes read (About 6151 words)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><h1 class="title is-3 is-size-4-mobile">TensorFlow</h1><div class="content"><p>…</p>
<span id="more"></span>

<h1 id="Tensorflow从零开始学"><a href="#Tensorflow从零开始学" class="headerlink" title="Tensorflow从零开始学"></a>Tensorflow从零开始学</h1><h2 id="1-机器学习基础"><a href="#1-机器学习基础" class="headerlink" title="1. 机器学习基础"></a>1. 机器学习基础</h2><blockquote>
<p>Tensorflow2.0 架构图</p>
</blockquote>
<img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/a03be577-3fec-4098-935a-187745943817.jpg" alt="img" style="zoom:50%;" />

<h3 id="1-1-机器学习分类"><a href="#1-1-机器学习分类" class="headerlink" title="1.1 机器学习分类"></a>1.1 机器学习分类</h3><p>机器学习主要分为以下几类：</p>
<ul>
<li>有监督学习(Supervised Learning)：根据有特征的数据进行训练、建模。包括分类、<strong>回归</strong>。常见算法有线性回归、逻辑回归、K-近邻朴素贝叶斯、决策树、随机森林、支持向量机。</li>
<li>无监督学习：直接建模。常见算法有K-means、EM</li>
<li>半监督学习</li>
<li>强化学习(Reinforcement Learning): 基于周围环境学习。</li>
</ul>
<img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/176dbf0f-1408-4ce7-9c9e-79f905cbd630.jpg" alt="img"  />

<h3 id="1-2-机器学习流程"><a href="#1-2-机器学习流程" class="headerlink" title="1.2 机器学习流程"></a>1.2 机器学习流程</h3><ol>
<li><strong>引入数据集</strong> &amp; <strong>数据预处理</strong> (特征工程：scikit-learn处理数据和提取特征)</li>
<li>**训练和测试模型 **(判断问题类型，选择算法和模型)</li>
<li><strong>评估模型</strong></li>
</ol>
<p><strong>数据预处理</strong>的几种常见简单方法：</p>
<ul>
<li><p>归一化：其作用包括无量纲化、加快模型的<strong>收敛速度</strong>等。使用<strong>min-max/Z-score</strong>算法。</p>
</li>
<li><p>标准化：将数据按比例缩放为一个限定区间。使用场景如距离度量、数据符合正态分布。</p>
</li>
<li><p>离散化：把连续的数值型数据进行很短。有助于消除异常数据，提高算法的效率。</p>
</li>
<li><p>二值化：简化数据，消除数据杂音。</p>
</li>
<li><p>哑编码：独热编码(One-Hot Encoding)。对特征(如形状、大小等)进行编码化。</p>
</li>
</ul>
<p><strong>特征工程</strong>：把原始数据<strong>转换</strong>为模型可用的数据。主要包括3方面：特征构造、特征提取和特征选择。</p>
<ul>
<li><p>特征构造：在原有特征的基础上做组合操作。例如对原有特征进行四则运算，从而得到新的特征。</p>
</li>
<li><p>特征提取：使用映射或变换的方法，降高维特征转换为低维的新特征。即降低维度（sklearn）。</p>
<p>常用方法有主成分分析（Principe Component Analysis，PCA）、线性判别分析（Linear Discriminant Analysis，LDA）和独立成分分析（Independent Component Analysis，ICA）</p>
<ul>
<li>PCA：无监督降维算法。主要实现是用“减少噪声”和“去冗余”来降维。</li>
<li>LDA：有监督降维算法。借助协方差矩阵、广义瑞利熵等实现类间距离的最大化和最小化。</li>
<li>ICA：降维的过程中保留相互独立的维度特征。</li>
</ul>
</li>
<li><p>特征选择：选择对模型影响大的特征，移除不太相关的特征。作用是<strong>减少过拟合</strong>、提高模型准确度、减少训练时间。</p>
<ul>
<li>过滤式：Pearson相关系数法、方差选择法、假设检验、互信息法等。</li>
<li>包裹式</li>
<li>嵌入式</li>
</ul>
</li>
</ul>
<p><strong>模型的评估和选择</strong></p>
<p>基本概念：</p>
<ul>
<li>参数：模型的内部变量。如模型的权重矩阵和偏置。</li>
<li>超参数：模型中可人为设定和修改的参数，或者称为训练过程中不变的参数，如神经网络的层数、学习率、隐藏层神经元的个数等。</li>
</ul>
<p>评估的<strong>常见方法</strong>：留出法、交叉验证法、留一法、自助法。</p>
<ul>
<li>留出法：按比例将数据集划分训练集、验证集、测试集<ul>
<li>验证集，是模型训练过程中单独留出的样本集，它可以用于调整模型的超参数和用于对模型的能力进行初步评估。 通常用来在模型迭代训练时，用以验证当前模型泛化能力（准确率，召回率等），以<strong>决定是否停止继续训练</strong>。</li>
</ul>
</li>
<li>交叉验证法：划分为k个数据集，其中一个作为验证集，剩下的k-1个作为训练集。将得到的k个结果取平均值，作为最终模型评估的结果。这种方法称为“k折交叉验证”。</li>
</ul>
<img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/d44d0b00-bcbc-4436-98e0-e4d3d28cb61d.jpg" alt="img" style="zoom:50%;" />

<p>模型的性能度量：正确率(Accuracy)、错误率(Error Rate)、查准率(Precision)、查全率/召回率(Recall)</p>
<img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/484bb6c3-efa2-4106-9e01-96215491f2d6.jpg" alt="img" style="zoom: 50%;" />
<img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/34c9ef7d-875a-4b93-b452-9699817e74a4.jpg" alt="img" style="zoom:50%;" />

<p>Precision和Recall的指标衡量使用F1公式。<br><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/b40d8531-b315-4d6d-8a4d-1bf85d727fcc.jpg" alt="img" style="zoom:50%;" /></p>
<h3 id="1-3-术语"><a href="#1-3-术语" class="headerlink" title="1.3 术语"></a>1.3 术语</h3><ul>
<li>计算图：动态计算图(Eager Execution)</li>
<li>会话</li>
<li>运算操作和运算核</li>
<li>张量(Tensor)：多维的数组或列表。</li>
</ul>
<img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/0d0b9db0-97c8-41f0-9d01-8f800f94296c.jpg" alt="img" style="zoom:50%;" />



<ul>
<li><p>Learning Rate</p>
<p>在<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Machine_learning">机器学习</a>和<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Statistics">统计学中</a>，<strong>学习率</strong>是<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Mathematical_optimization">优化算法</a>中的一个<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)">调整参数</a>，它确定每次迭代时<strong>梯度下降的步长</strong>，同时朝着<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Loss_function">损失函数</a>的最小值移动。[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Learning_rate#cite_note-1">1]</a>由于它会影响新获取的信息覆盖旧信息的程度，因此它隐喻地代表了机器学习模型“学习”的速度。在<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Adaptive_control">自适应控制</a>文献中，学习率通常称为<strong>增益</strong>。[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Learning_rate#cite_note-2">2]</a></p>
<p>如果知道感知机原理的话，那很快就能知道，Learning Rate是调整神经网络输入权重的一种方法。如果感知机预测正确，则对应的输入权重不会变化，否则会根据Loss Function来对感知机重新调整，而这个调整的幅度大小就是Learning Rate，也就是在调整的基础上，增加一个比值。</p>
<p>如下图的权重w，在输出之后预测正确与否，若正确则保持权重w不变，若错误则用右边的公式来对权重w进行调整，如下图：</p>
<p><img src="https://pic3.zhimg.com/80/v2-d4563539367d5cfcc3ff749eb309d45a_1440w.jpg" alt="img"></p>
<p>那一般Learning Rate的取值都在0.0001到0.01之间，这个效果就像你走路的步子，步子迈达了，很容易错过最佳点(指使loss损失函数为最小值时的 LR)，而迈小了又需要花更长的时间才能到最佳点。</p>
<p><img src="https://pic3.zhimg.com/80/v2-28aeed7f2b1363dfbd3c55cc2360868e_1440w.jpg" alt="img"></p>
<p>所以，直观上来看，在这最好的办法是：先走快一些（大一些的Learning Rate），然后要到最佳点的时候，再降低Learning Rate来到达最佳点。</p>
</li>
<li><p>梯度下降(Gradient Descent)算法</p>
</li>
<li><p>loss 和 val_loss : loss 是训练集的损失值，val_loss(validation) 是验证集的损失值。</p>
<p>以下是loss与val_loss的变化反映出训练走向的规律总结：</p>
<p>train loss 不断下降，test loss不断下降，说明网络仍在学习;（最好的）</p>
<p>train loss 不断下降，test loss趋于不变，说明网络过拟合;（max pool或者正则化）</p>
<p>train loss 趋于不变，test loss不断下降，说明数据集100%有问题;（检查dataset）</p>
<p>train loss 趋于不变，test loss趋于不变，说明学习遇到瓶颈，需要<strong>减小学习率</strong>或批量数目;（减少学习率）</p>
<p>train loss 不断上升，test loss不断上升，说明网络结构设计不当，训练<strong>超参数</strong>设置不当，数据集经过清洗等问题。（最不好的情况）</p>
</li>
</ul>
<h3 id="1-4-Wide-amp-Deep-模型"><a href="#1-4-Wide-amp-Deep-模型" class="headerlink" title="1.4 Wide&amp;Deep 模型"></a>1.4 Wide&amp;Deep 模型</h3><blockquote>
<p>论文地址(Wide &amp; Deep Learning for Recommender Systems )：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1606.07792v1.pdf">https://arxiv.org/pdf/1606.07792v1.pdf</a></p>
</blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/5b807e6c3801">https://www.jianshu.com/p/5b807e6c3801</a></p>
</blockquote>
<p>稀疏特征+密集特征</p>
<p><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/9YRKFZ.png" alt="9YRKFZ"></p>
<img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/Q1M5Dj.png" alt="Q1M5Dj" style="zoom: 50%;" />

<h2 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h2><h3 id="2-1-归一化"><a href="#2-1-归一化" class="headerlink" title="2.1 归一化"></a>2.1 归一化</h3><p><strong>归一化：</strong>将数据集转换为(0,1)区间范围或者无量纲化(dimensionless，去维度)，使其位于激活函数有明显梯度的位置，才能使每一层神经网络之间感知到明显变化的loss损失函数值。如果在梯度不明显的地方(如函数平缓，即导数接近0；神经网络层数多，导致每一层神经网络的loss变化不大)，短时间内的训练效果会不明显。下面是 2 种归一化方法（均值MSE，方差STD）：</p>
<img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/V5FA0J.png" alt="V5FA0J" style="zoom:50%;" />

<p><strong>批归一化：</strong>对每层神经训练后的数据再进行归一化。参考文章：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/skyfsm/p/8453498.html">https://www.cnblogs.com/skyfsm/p/8453498.html</a></p>
<img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/74CNoT.png" alt="74CNoT" style="zoom:50%;" />

<p>a中左图是没有经过任何处理的输入数据，曲线是sigmoid函数，如果数据在梯度很小的区域，那么学习率就会很慢甚至陷入长时间的停滞。减均值除方差后，数据就被移到中心区域如右图所示，对于大多数激活函数而言，这个区域的梯度都是最大的或者是有梯度的（比如ReLU），这可以看做是一种对抗梯度消失的有效手段。</p>
<h3 id="2-2-One-Hot-编码"><a href="#2-2-One-Hot-编码" class="headerlink" title="2.2 One-Hot 编码"></a>2.2 One-Hot 编码</h3><p>作为机器学习算法的输入，通常需要对其进行特征数字化。故使用one-hot编码可以将具备离散值特征的<strong>分类数据</strong>转化为<strong>二进制向量</strong>。例如，按照 N位状态寄存器 来 对N个状态 进行编码的原理，处理后应该是这样的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">性别特征：[&quot;男&quot;,&quot;女&quot;] （这里只有两个特征，所以 N&#x3D;2）：</span><br><span class="line"></span><br><span class="line">男  &#x3D;&gt;  10</span><br><span class="line"></span><br><span class="line">女  &#x3D;&gt;  01</span><br><span class="line"></span><br><span class="line">祖国特征：[&quot;中国&quot;，&quot;美国，&quot;法国&quot;]（N&#x3D;3）：</span><br><span class="line"></span><br><span class="line">中国  &#x3D;&gt;  100</span><br><span class="line"></span><br><span class="line">美国  &#x3D;&gt;  010</span><br><span class="line"></span><br><span class="line">法国  &#x3D;&gt;  001</span><br></pre></td></tr></table></figure>

<h2 id="3-前馈神经网络"><a href="#3-前馈神经网络" class="headerlink" title="3. 前馈神经网络"></a>3. 前馈神经网络</h2><h3 id="3-1-神经网络"><a href="#3-1-神经网络" class="headerlink" title="3.1 神经网络"></a>3.1 神经网络</h3><blockquote>
<p>感知器模型、多层神经网络</p>
</blockquote>
<p>感知器模型(单层神经网络，是一种两类<strong>线性</strong>分类模型)</p>
<p><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/97c1df15-263b-476a-8b1f-914e3d77e667.jpg" alt="img"><br><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/4f1f437b-9aee-49a0-b4b3-bf8042d9e69b.jpg" alt="img"></p>
<p>多层神经网络（可通过隐藏层的<strong>非线性激活函数</strong>，解决非线性问题）</p>
<p><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/c4340a6f-332c-4866-8eb8-5ed76835a4e5.jpg" alt="img"><br><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/e5dd865c-7634-47af-a32a-6daa5f83c2d3.jpg" alt="img"></p>
<h3 id="3-2-激活函数"><a href="#3-2-激活函数" class="headerlink" title="3.2 激活函数"></a>3.2 激活函数</h3><h4 id="激活函数的概念和作用"><a href="#激活函数的概念和作用" class="headerlink" title="激活函数的概念和作用"></a>激活函数的概念和作用</h4><blockquote>
<p>为什么要用激活函数？</p>
<p>如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%84%9F%E7%9F%A5%E6%9C%BA/12723581">感知机</a>（Perceptron）。<br>如果使用的话，激活函数给神经元引入了<strong>非线性</strong>因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。</p>
<p>为了解决非线性的分类或回归问题，激活函数必须是非线性函数。另外使用基于梯度(慢慢趋于某个值，才能分类)的方式来训练模型，因此激活函数也必须是<strong>连续可导</strong>的。</p>
<p>激活函数：Logistic(Sigmoid)函数、Tanh函数、ReLU函数(常用)</p>
</blockquote>
<p><strong>激活函数：</strong></p>
<p><strong>梯度消失：</strong>随着神经网络层数的加深，梯度后向传播到浅层网络时，基本无法引起参数的扰动，也就是没有将loss的信息传递到浅层网络，这样网络就无法训练学习了。(即随着隐藏层数目的增加，分类准确率反而下降了；更为直白的表达，就是梯度消失，函数成为横线变为不可导。)</p>
<p>Cause：Sig函数求导后的导数最大值为0.25(当x=0时)。神经网络主要的训练方法是BP算法，而反向传播算法(Back Propagation)的基础是导数的链式法则，也就是多个导数的乘积。当层数过多时，导数乘积会变得非常小，就会造成梯度消失现象了。</p>
<p><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/6ea95886-2432-4644-b7dd-5675183d11b2.jpg" alt="img"><br><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/231b174f-5c32-427f-8927-945bf0065613.jpg" alt="img"><br><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/297faa82-c1e4-4ed7-b7a8-4698da23c001.jpg" alt="img"></p>
<h4 id="Relu-衍生函数"><a href="#Relu-衍生函数" class="headerlink" title="Relu 衍生函数"></a>Relu 衍生函数</h4><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/rFPtQR.png" alt="rFPtQR" style="zoom:50%;" />



<h3 id="3-3-输出单元"><a href="#3-3-输出单元" class="headerlink" title="3.3 输出单元"></a>3.3 输出单元</h3><p><strong>输出单元：</strong>1.线性单元(回归) 2.Sigmoid单元(二分类) 3.Softmax单元(多分类)</p>
<p><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/cb2630be-bf11-4045-98b7-cf91354ca9ed.jpg" alt="img"><br><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/1ff76ca3-b5cc-4624-8540-780b492f6e0d.jpg" alt="img"><br><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/e46dab09-d0f3-453d-9823-565da16c98c1.jpg" alt="img"></p>
<h3 id="3-4-损失函数"><a href="#3-4-损失函数" class="headerlink" title="3.4 损失函数"></a>3.4 损失函数</h3><blockquote>
<p><strong>损失函数(目标函数)**用来表达模型的</strong>预测值和真实<strong>类标之间的误差。<br>深度学习模型的训练就是使用基于梯度的方法</strong>使损失函数最小化**的过程。<br>损失函数和输出单元有密切的关系？</p>
</blockquote>
<p><strong>损失函数：</strong></p>
<p>1.均方误差损失函数(Mean Squared Error, MSE)，用于回归问题。</p>
<p>2.交叉熵损失函数(Cross Entropy)，用于分类问题。</p>
<p><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/00ede66f-5ce3-444c-a323-9cb1c027f9f7.jpg" alt="img"><br><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/ea684877-1a0c-4fd5-a3f5-f037508ddf23.jpg" alt="img"></p>
<p><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/lpmGcn.png" alt="lpmGcn"></p>
<h3 id="3-5-优化器"><a href="#3-5-优化器" class="headerlink" title="3.5 优化器"></a>3.5 优化器</h3><ol>
<li>sgd：随机梯度下降</li>
<li>adam：</li>
<li>RMSProp：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34230849">https://zhuanlan.zhihu.com/p/34230849</a></li>
</ol>
<h3 id="3-6-示例：MNIST手写数字识别"><a href="#3-6-示例：MNIST手写数字识别" class="headerlink" title="3.6 示例：MNIST手写数字识别"></a>3.6 示例：MNIST手写数字识别</h3><p>这是一个有监督的分类问题。（此处指类标label，而不是类别category）</p>
<h2 id="4-卷积神经网络"><a href="#4-卷积神经网络" class="headerlink" title="4. 卷积神经网络"></a>4. 卷积神经网络</h2><blockquote>
<p>卷积神经网络(Convolutional Neural Network，CNN)是一种专门用来处理<strong>网格结构数据</strong>(如图像数据)的前馈神经网络。相对全连接神经网络<strong>参数更少</strong>，减少了模型训练的复杂性和过拟合问题，使模型的泛化能力更强。</p>
</blockquote>
<h3 id="4-1-CNN的基本特征与结构"><a href="#4-1-CNN的基本特征与结构" class="headerlink" title="4.1 CNN的基本特征与结构"></a>4.1 CNN的基本特征与结构</h3><p><strong>CNN主要有3大特征</strong>：1.局部连接 2.权值共享 3.子采样(池化)</p>
<p><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/a6d00b83-1994-4fe7-bdfc-bce6e9ee68a7.jpg" alt="img"><br><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/02fd8c12-9cf5-4278-827e-0b7b32691cb0.jpg" alt="img"></p>
<h3 id="4-2-卷积层"><a href="#4-2-卷积层" class="headerlink" title="4.2 卷积层"></a>4.2 卷积层</h3><p>卷积公式：<br><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/792f1e1f-b5cd-4ff5-a90b-0deb93578f91.jpg" alt="img"></p>
<p>卷积的示例：<br><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/54ca4ddb-9fbf-4ded-a517-dac94e31fb69.jpg" alt="img"><br><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/a01324c7-fb09-44a2-91c5-4c24391ab722.jpg" alt="img"></p>
<p>卷积的作用：<br>将各种低级特征(输入层的图像的特征)卷积成高级特征(特征图)。</p>
<p><strong>卷积层的基本结构</strong><br><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/3d804594-4599-4640-ac5d-534efb74443d.jpg" alt="img"><br><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/d816ff44-ac35-4289-a083-a62e77b1390e.jpg" alt="img"></p>
<h3 id="4-3-池化层"><a href="#4-3-池化层" class="headerlink" title="4.3 池化层"></a>4.3 池化层</h3><blockquote>
<p>池化层(Pooling Layer)也称为子采样层(Subsampling Layer)，一般都紧跟在卷积层之后，它的作用是进行特征选择(对特征图进行采样)，减少特征的数量，从而减少网络参数的数量。</p>
</blockquote>
<p><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/f01c1061-277c-4bde-bcf9-7021aa1db659.jpg" alt="img"></p>
<p>池化层的过程其实在卷积层也能完成。</p>
<h3 id="4-4-CNN图像分类"><a href="#4-4-CNN图像分类" class="headerlink" title="4.4 CNN图像分类"></a>4.4 CNN图像分类</h3><h2 id="5-循环神经网络"><a href="#5-循环神经网络" class="headerlink" title="5. 循环神经网络"></a>5. 循环神经网络</h2><blockquote>
<p>循环神经网络(Recurrent Neural Network, RNN)，用于解决具有数据时序前后关联性(如文本、语音、视频、无人驾驶等)的问题。<br>强化学习应该是实时的。</p>
</blockquote>
<h3 id="5-1-RNN"><a href="#5-1-RNN" class="headerlink" title="5.1 RNN"></a>5.1 RNN</h3><blockquote>
<p>RNN的基本结构、前面计算的过程、参数优化的问题</p>
</blockquote>
<p><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/c12a1b21-df1c-485b-a362-32a4631552d4.jpg" alt="img"></p>
<p><strong>记忆单元</strong>能记住有限时间范围内的信息，同时能将这些信息传递到下一层或上一层的神经元（即每个时刻的输出是由上下两个循环神经网络的输出共同决定的）。</p>
<blockquote>
<p>梯度消失和梯度爆炸、解决方法(梯度截断、正则化及改进模型等)</p>
</blockquote>
<p><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/ca0b9d7f-12e8-4d59-b2be-c1abd4aa23a7.jpg" alt="img"><br><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/0a291647-0c1a-4a62-9eb4-306554975f3e.jpg" alt="img"></p>
<h3 id="5-2-门控RNN"><a href="#5-2-门控RNN" class="headerlink" title="5.2 门控RNN"></a>5.2 门控RNN</h3><blockquote>
<p>在RNN基础上增加门控机制，用来控制神经网络中信息的传递。即保留或丢弃记忆单元中的信息。<br>这样就可以使RNN学习时间跨度更大的依赖关系，而不会出现梯度消失和梯度爆炸问题。</p>
</blockquote>
<p><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/67e43065-88ff-4e0e-ab0e-65c70676156b.jpg" alt="img"></p>
<p><strong>长短期记忆网络</strong>(Long Short-Term Memory, LSTM):当趋于梯度消失时，被遗忘的信息越多；当趋于梯度爆炸时，记忆的信息越多。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.LSTM</span><br></pre></td></tr></table></figure>

<p><strong>门控循环单元</strong>(Gated Recurrent Unit, GRU)：GRU网络结构比LSTM简单，它将LSTM的输入门和遗忘门合并为更新门。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.GRU</span><br></pre></td></tr></table></figure>

<h3 id="5-3-RNN的应用"><a href="#5-3-RNN的应用" class="headerlink" title="5.3 RNN的应用"></a>5.3 RNN的应用</h3><blockquote>
<p>应用场景主要有：自然语言处理(Natural Language Processing，NLP)、语音识别/合成、聊天机器人、机器翻译等。</p>
</blockquote>
<h3 id="5-4-注意力模型（强化学习？）"><a href="#5-4-注意力模型（强化学习？）" class="headerlink" title="5.4 注意力模型（强化学习？）"></a>5.4 注意力模型（强化学习？）</h3><p>跳过</p>
<h2 id="6-深度强化学习"><a href="#6-深度强化学习" class="headerlink" title="6. 深度强化学习"></a>6. 深度强化学习</h2><p><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/167f9296-e34d-478c-828c-deca8b2a404d.jpg" alt="img"></p>
<h3 id="6-1-基础知识"><a href="#6-1-基础知识" class="headerlink" title="6.1 基础知识"></a>6.1 基础知识</h3><blockquote>
<p>行为模式：Do &amp; Correct<br>动作空间(Action Space)：A<br>状态空间(State Space)：S<br>奖励(Reward)：R<br>状态转移概率矩阵(Transition)：P</p>
</blockquote>
<p>强化学习和传统的有监督学习的区别：有监督学习 Learning with a teacher，强化学习 Learning with a critic</p>
<h3 id="6-2-有模型的强化学习方法"><a href="#6-2-有模型的强化学习方法" class="headerlink" title="6.2 有模型的强化学习方法"></a>6.2 有模型的强化学习方法</h3><h3 id="6-3-无模型的强化学习方法"><a href="#6-3-无模型的强化学习方法" class="headerlink" title="6.3 无模型的强化学习方法"></a>6.3 无模型的强化学习方法</h3><h3 id="6-4-强化学习算法"><a href="#6-4-强化学习算法" class="headerlink" title="6.4 强化学习算法"></a>6.4 强化学习算法</h3><h3 id="6-5-深度强化学习算法"><a href="#6-5-深度强化学习算法" class="headerlink" title="6.5 深度强化学习算法"></a>6.5 深度强化学习算法</h3><h2 id="7-项目实战"><a href="#7-项目实战" class="headerlink" title="7. 项目实战"></a>7. 项目实战</h2><h2 id="8-Tensorflow-代码"><a href="#8-Tensorflow-代码" class="headerlink" title="8. Tensorflow 代码"></a>8. Tensorflow 代码</h2><h3 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h3><p>视频 3-6子类自定义</p>
<p>layer 3-9签名函数和图结构</p>
<p>3-12手动metrics mse</p>
<p>第4章csv</p>
<p><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/bF4W8x.png" alt="bF4W8x"></p>
<h3 id="8-1-回归和分类"><a href="#8-1-回归和分类" class="headerlink" title="8.1 回归和分类"></a>8.1 回归和分类</h3><p>本笔记本 (notebook) 介绍了一些处理回归问题的技术。</p>
<ul>
<li>均方误差（MSE）是用于回归问题的常见损失函数（分类问题中使用不同的损失函数）。</li>
<li>类似的，用于回归的评估指标与分类不同。 常见的回归指标是平均绝对误差（MAE）。</li>
<li>当数字输入数据特征的值存在不同范围时，每个特征应独立缩放到相同范围。</li>
<li>如果训练数据不多，一种方法是选择隐藏层较少的小网络，以避免过度拟合。</li>
<li>早期停止是一种防止过度拟合的有效技术。</li>
</ul>
<h3 id="8-2-TF-基础-API"><a href="#8-2-TF-基础-API" class="headerlink" title="8.2 TF 基础 API"></a>8.2 TF 基础 API</h3><h4 id="tf-constant"><a href="#tf-constant" class="headerlink" title="tf.constant()"></a>tf.constant()</h4><h2 id="9-Q-amp-A"><a href="#9-Q-amp-A" class="headerlink" title="9. Q&amp;A"></a>9. Q&amp;A</h2><h3 id="9-1-数据降维"><a href="#9-1-数据降维" class="headerlink" title="9.1 数据降维"></a>9.1 数据降维</h3><p><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/84b6678c-1988-47e1-a724-cce1b52165fd.png" alt="img"></p>
<h3 id="9-2-防止过拟合"><a href="#9-2-防止过拟合" class="headerlink" title="9.2 防止过拟合"></a>9.2 防止过拟合</h3><p>todo: <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/tutorials/keras/overfit_and_underfit?hl=zh_cn">https://tensorflow.google.cn/tutorials/keras/overfit_and_underfit?hl=zh_cn</a></p>
<p>Dropout、取更重要的特征，忽略不重要的特征、early stopping</p>
<h2 id="10-TensorFlow-官网教程"><a href="#10-TensorFlow-官网教程" class="headerlink" title="10. TensorFlow 官网教程"></a>10. TensorFlow 官网教程</h2><p>Todo</p>
<p><a target="_blank" rel="noopener" href="https://keras.io/examples/">https://keras.io/examples/</a></p>
<p><img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/g2zZqu.png" alt="g2zZqu"></p>
<h3 id="10-1-快速开始"><a href="#10-1-快速开始" class="headerlink" title="10.1 快速开始"></a>10.1 快速开始</h3><h3 id="10-2-Keras-机器学习基础知识"><a href="#10-2-Keras-机器学习基础知识" class="headerlink" title="10.2 Keras 机器学习基础知识"></a>10.2 Keras 机器学习基础知识</h3><h4 id="基本图像分类"><a href="#基本图像分类" class="headerlink" title="基本图像分类"></a>基本图像分类</h4><h4 id="基本文本分类"><a href="#基本文本分类" class="headerlink" title="基本文本分类"></a>基本文本分类</h4><h4 id="使用-TF-Hub-进行文本分类"><a href="#使用-TF-Hub-进行文本分类" class="headerlink" title="使用 TF Hub 进行文本分类"></a>使用 TF Hub 进行文本分类</h4><p><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/hub">TensorFlow Hub</a>，一个用于迁移学习的库和平台。关于迁移学习可参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/dakenz/article/details/85954548">该文章</a>。</p>
<h4 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h4><h4 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h4><p>防止过度拟合的最简单方法是从一个小模型开始：一个具有少量可学习参数（由层数和每层单元数决定）的模型。在深度学习中，模型中可学习参数的数量通常被称为模型的“容量”。</p>
<p>L1、L2 正则化本质上也是使简化模型，使其具有较少参数。</p>
<p><code>&quot;L2&quot;</code>正则化模型现在比<code>&quot;Tiny&quot;</code>模型更具竞争力。这种<code>&quot;L2&quot;</code>模式也更耐比过拟合<code>&quot;Large&quot;</code>，尽管有相同数量的参数，它是基于模型。</p>
<img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/shhpOQ.png" alt="shhpOQ" style="zoom:50%;" />

<p>Dropout 是最有效和最常用的神经网络正则化技术之一，由 Hinton 和他在多伦多大学的学生开发。</p>
<p>对 dropout 的直观解释是，由于网络中的单个节点不能依赖其他节点的输出，因此每个节点必须输出对自己有用的特征。</p>
<p>应用于层的 Dropout 包括在训练期间随机“丢弃”（即设置为零）该层的许多输出特征。假设一个给定的层通常会在训练期间为给定的输入样本返回一个向量 [0.2, 0.5, 1.3, 0.8, 1.1] ；应用 dropout 后，该向量将随机分布一些零条目，例如 [0, 0.5, 1.3, 0, 1.1]。</p>
<p>“辍学率”是被归零的特征的比例；它通常设置在 0.2 到 0.5 之间。在测试时，没有单元被丢弃，而是层的输出值按与丢弃率相等的因子缩小，以平衡比训练时更多的单元处于活动状态这一事实。</p>
<img src="https://gitee.com/jacky_cloud/oss/raw/master/uPic/tXty8y.png" alt="tXty8y" style="zoom:50%;" />

<p>过拟合的反面是<em>欠</em>拟合。当训练数据仍有改进空间时，就会发生欠拟合。发生这种情况的原因有很多：如果模型不够强大、过度规范化，或者只是训练时间不够长。这意味着网络还没有学习到训练数据中的相关模式。</p>
<p><strong>总结</strong></p>
<p>回顾一下：以下是防止神经网络过度拟合的最常见方法：</p>
<ul>
<li>获取更多训练数据。</li>
<li>减少网络容量(模型参数)。</li>
<li>添加权重正则化(Weight Regularization)。</li>
<li>添加辍学。</li>
</ul>
<p>本指南未涵盖的两种重要方法是：</p>
<ul>
<li>数据增强</li>
<li>批量标准化(Batch Normalization)</li>
</ul>
<p>请记住，每种方法都可以单独提供帮助，但通常将它们结合起来会更有效。</p>
<h4 id="保存和加载"><a href="#保存和加载" class="headerlink" title="保存和加载"></a>保存和加载</h4><p>选项</p>
<p>保存 Tensorflow 的模型有许多方法——具体取决于您使用的 API。本指南使用 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/guide/keras">tf.keras</a>， 一个高级 API 用于在 Tensorflow 中构建和训练模型。有关其他方法的实现，请参阅 TensorFlow <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/guide/saved_model">保存和恢复</a>指南或<a target="_blank" rel="noopener" href="https://tensorflow.google.cn/guide/eager#object-based_saving">保存到 eager</a>。</p>
<p>Keras 通过检查网络结构来保存模型。这项技术可以保存一切:</p>
<ul>
<li>权重值</li>
<li>模型的架构</li>
<li>模型的训练配置(您传递给编译的内容)</li>
<li>优化器及其状态（如果有的话）（这使您可以在中断的地方重新开始训练）</li>
</ul>
<p>Keras 无法保存 <code>v1.x</code> 优化器（来自 <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/train"><code>tf.compat.v1.train</code></a>），因为它们与检查点不兼容。对于 v1.x 优化器，您需要在加载-失去优化器的状态后，重新编译模型。</p>
<p>如果使用的是 SavedModel 格式，则可以跳过此部分。HDF5 和 SavedModel 之间的主要区别在于，HDF5 使用对象配置保存模型结构，而 SavedModel 保存执行图。因此，SavedModel 能够保存自定义对象，例如子类化模型和自定义层，而无需原始代码。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;www.tensorflow.org&#x2F;tutorials&#x2F;keras&#x2F;save_and_load#%E4%BF%9D%E5%AD%98%E8%87%AA%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%B1%A1</span><br></pre></td></tr></table></figure>



<h4 id="使用-Keras-Tuner-调整超参数"><a href="#使用-Keras-Tuner-调整超参数" class="headerlink" title="使用 Keras Tuner 调整超参数"></a>使用 Keras Tuner 调整超参数</h4><p>超参数有两种类型：</p>
<ol>
<li>影响模型选择的<strong>模型超参数</strong>，例如隐藏层的数量和宽度 to do 隐藏层是什么</li>
<li>影响学习算法速度和质量的<strong>算法超参数</strong>，例如随机梯度下降 (SGD) 的学习率和 ak 最近邻 (KNN) 分类器的最近邻数</li>
</ol>
<p>您可以通过两种方法定义超模型：</p>
<ul>
<li>通过使用模型构建器功能 here</li>
<li>通过<code>HyperModel</code>继承 Keras Tuner API的类</li>
</ul>
<p>The Keras Tuner has four tuners available - <code>RandomSearch</code>, <code>Hyperband</code>, <code>BayesianOptimization</code>, and <code>Sklearn</code></p>
<p><strong>概括</strong></p>
<p>在本教程中，您学习了如何使用 Keras Tuner 调整模型的超参数。要了解有关 Keras Tuner 的更多信息，请查看以下附加资源：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html">TensorFlow 博客上的 Keras Tuner</a></li>
<li><a target="_blank" rel="noopener" href="https://keras-team.github.io/keras-tuner/">Keras Tuner website</a></li>
</ul>
<p>Also check out the <a target="_blank" rel="noopener" href="https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams">HParams Dashboard</a> in TensorBoard to interactively tune your model hyperparameters.</p>
<h3 id="10-3-加载和预处理数据"><a href="#10-3-加载和预处理数据" class="headerlink" title="10.3 加载和预处理数据"></a>10.3 加载和预处理数据</h3><h4 id="图像"><a href="#图像" class="headerlink" title="图像"></a>图像</h4><h4 id="CSV"><a href="#CSV" class="headerlink" title="CSV"></a>CSV</h4><h4 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h4><h4 id="pandas-DataFrame"><a href="#pandas-DataFrame" class="headerlink" title="pandas.DataFrame"></a>pandas.DataFrame</h4><h4 id="文本"><a href="#文本" class="headerlink" title="文本"></a>文本</h4><h4 id="Unicode"><a href="#Unicode" class="headerlink" title="Unicode"></a>Unicode</h4><h4 id="TF-Text"><a href="#TF-Text" class="headerlink" title="TF.Text"></a>TF.Text</h4><h4 id="子词分词化"><a href="#子词分词化" class="headerlink" title="子词分词化"></a>子词分词化</h4><h4 id="TFRecord-和-tf-Example"><a href="#TFRecord-和-tf-Example" class="headerlink" title="TFRecord 和 tf.Example"></a>TFRecord 和 tf.Example</h4></div><div class="article-licensing box"><div class="licensing-title"><p>TensorFlow</p><p><a href="https://teamwang.cn/2021/09/03/TensorFlow/">https://teamwang.cn/2021/09/03/TensorFlow/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Jacky</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2021-09-03</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-09-03</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=60364d8c1dae0d00189a79e8&amp;product=inline-share-buttons" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/04/14/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-%E7%AE%97%E6%B3%95/"><span class="level-item">操作系统&amp;计算机网络&amp;算法</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="content" id="valine-thread"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script><script>new Valine({
            el: '#valine-thread' ,
            appId: "D7nrmJ3xSHla6mxR3p0BrXqD-gzGzoHsz",
            appKey: "rGXO6glIn8ETvzytN7maFaoP",
            placeholder: "Leave your comments~",
            
            
            meta: ["nick"],
            pageSize: 10,
            lang: "en",
            visitor: true,
            highlight: true,
            
            
            
            
            enableQQ: true,
            requiredFields: [],
        });</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Tensorflow从零开始学"><span class="level-left"><span class="level-item">Tensorflow从零开始学</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-机器学习基础"><span class="level-left"><span class="level-item">1. 机器学习基础</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-1-机器学习分类"><span class="level-left"><span class="level-item">1.1 机器学习分类</span></span></a></li><li><a class="level is-mobile" href="#1-2-机器学习流程"><span class="level-left"><span class="level-item">1.2 机器学习流程</span></span></a></li><li><a class="level is-mobile" href="#1-3-术语"><span class="level-left"><span class="level-item">1.3 术语</span></span></a></li><li><a class="level is-mobile" href="#1-4-Wide-amp-Deep-模型"><span class="level-left"><span class="level-item">1.4 Wide&amp;Deep 模型</span></span></a></li></ul></li><li><a class="level is-mobile" href="#2-数据预处理"><span class="level-left"><span class="level-item">2. 数据预处理</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#2-1-归一化"><span class="level-left"><span class="level-item">2.1 归一化</span></span></a></li><li><a class="level is-mobile" href="#2-2-One-Hot-编码"><span class="level-left"><span class="level-item">2.2 One-Hot 编码</span></span></a></li></ul></li><li><a class="level is-mobile" href="#3-前馈神经网络"><span class="level-left"><span class="level-item">3. 前馈神经网络</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#3-1-神经网络"><span class="level-left"><span class="level-item">3.1 神经网络</span></span></a></li><li><a class="level is-mobile" href="#3-2-激活函数"><span class="level-left"><span class="level-item">3.2 激活函数</span></span></a></li><li><a class="level is-mobile" href="#3-3-输出单元"><span class="level-left"><span class="level-item">3.3 输出单元</span></span></a></li><li><a class="level is-mobile" href="#3-4-损失函数"><span class="level-left"><span class="level-item">3.4 损失函数</span></span></a></li><li><a class="level is-mobile" href="#3-5-优化器"><span class="level-left"><span class="level-item">3.5 优化器</span></span></a></li><li><a class="level is-mobile" href="#3-6-示例：MNIST手写数字识别"><span class="level-left"><span class="level-item">3.6 示例：MNIST手写数字识别</span></span></a></li></ul></li><li><a class="level is-mobile" href="#4-卷积神经网络"><span class="level-left"><span class="level-item">4. 卷积神经网络</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#4-1-CNN的基本特征与结构"><span class="level-left"><span class="level-item">4.1 CNN的基本特征与结构</span></span></a></li><li><a class="level is-mobile" href="#4-2-卷积层"><span class="level-left"><span class="level-item">4.2 卷积层</span></span></a></li><li><a class="level is-mobile" href="#4-3-池化层"><span class="level-left"><span class="level-item">4.3 池化层</span></span></a></li><li><a class="level is-mobile" href="#4-4-CNN图像分类"><span class="level-left"><span class="level-item">4.4 CNN图像分类</span></span></a></li></ul></li><li><a class="level is-mobile" href="#5-循环神经网络"><span class="level-left"><span class="level-item">5. 循环神经网络</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#5-1-RNN"><span class="level-left"><span class="level-item">5.1 RNN</span></span></a></li><li><a class="level is-mobile" href="#5-2-门控RNN"><span class="level-left"><span class="level-item">5.2 门控RNN</span></span></a></li><li><a class="level is-mobile" href="#5-3-RNN的应用"><span class="level-left"><span class="level-item">5.3 RNN的应用</span></span></a></li><li><a class="level is-mobile" href="#5-4-注意力模型（强化学习？）"><span class="level-left"><span class="level-item">5.4 注意力模型（强化学习？）</span></span></a></li></ul></li><li><a class="level is-mobile" href="#6-深度强化学习"><span class="level-left"><span class="level-item">6. 深度强化学习</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#6-1-基础知识"><span class="level-left"><span class="level-item">6.1 基础知识</span></span></a></li><li><a class="level is-mobile" href="#6-2-有模型的强化学习方法"><span class="level-left"><span class="level-item">6.2 有模型的强化学习方法</span></span></a></li><li><a class="level is-mobile" href="#6-3-无模型的强化学习方法"><span class="level-left"><span class="level-item">6.3 无模型的强化学习方法</span></span></a></li><li><a class="level is-mobile" href="#6-4-强化学习算法"><span class="level-left"><span class="level-item">6.4 强化学习算法</span></span></a></li><li><a class="level is-mobile" href="#6-5-深度强化学习算法"><span class="level-left"><span class="level-item">6.5 深度强化学习算法</span></span></a></li></ul></li><li><a class="level is-mobile" href="#7-项目实战"><span class="level-left"><span class="level-item">7. 项目实战</span></span></a></li><li><a class="level is-mobile" href="#8-Tensorflow-代码"><span class="level-left"><span class="level-item">8. Tensorflow 代码</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#TODO"><span class="level-left"><span class="level-item">TODO</span></span></a></li><li><a class="level is-mobile" href="#8-1-回归和分类"><span class="level-left"><span class="level-item">8.1 回归和分类</span></span></a></li><li><a class="level is-mobile" href="#8-2-TF-基础-API"><span class="level-left"><span class="level-item">8.2 TF 基础 API</span></span></a></li></ul></li><li><a class="level is-mobile" href="#9-Q-amp-A"><span class="level-left"><span class="level-item">9. Q&amp;A</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#9-1-数据降维"><span class="level-left"><span class="level-item">9.1 数据降维</span></span></a></li><li><a class="level is-mobile" href="#9-2-防止过拟合"><span class="level-left"><span class="level-item">9.2 防止过拟合</span></span></a></li></ul></li><li><a class="level is-mobile" href="#10-TensorFlow-官网教程"><span class="level-left"><span class="level-item">10. TensorFlow 官网教程</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#10-1-快速开始"><span class="level-left"><span class="level-item">10.1 快速开始</span></span></a></li><li><a class="level is-mobile" href="#10-2-Keras-机器学习基础知识"><span class="level-left"><span class="level-item">10.2 Keras 机器学习基础知识</span></span></a></li><li><a class="level is-mobile" href="#10-3-加载和预处理数据"><span class="level-left"><span class="level-item">10.3 加载和预处理数据</span></span></a></li></ul></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Jacky"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jacky</p><p class="is-size-6 is-block">(ง ˙o˙)ว</p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">23</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">10</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tag</p><a href="/tags"><p class="title">1</p></a></div></div></nav><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="https://weibo.com/"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/cloris-cc"><i class="fab fa-github"></i></a></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><p class="is-size-7"><span>&copy; 2021 Jacky</span>  <a href="https://beian.miit.gov.cn/" target="_blank">粤ICP备19098392号</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Weibo" href="https://weibo.com/"><i class="fab fa-weibo"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/cloris-cc"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>